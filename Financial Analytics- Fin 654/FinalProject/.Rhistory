p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) +
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind]))
p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3]) + annotate("text", x = sd.R[4], y = mean.R[4], label = names.R[4])
p	### ggplotly(p)
})
# Chunk 28
weights[ind2,]
# Chunk 29
weights[ind,]
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=36))
knitr::opts_chunk$set(size = "small")
knitr::opts_hooks$set(fig.width = function(options) {
if (options$fig.width < options$fig.height) {
options$fig.width = options$fig.height
}
options
})
knitr::knit_hooks$set(mysize = function(before, options, envir) {
if (before)
return(options$size)
})
# Chunk 2
require(ggplot2)
require(flexdashboard)
require(shiny)
require(QRM)
require(qrmdata)
require(xts)
require(zoo)
require(psych)
library(lubridate)
library(plyr)
library(plotly)
#library(ggfortify)
library(psych)
require(moments)
require(matrixStats)
require(digest)
require(Rcpp)
CompEx = read.csv('ExxonStockPrices.csv', header=TRUE)
CompShell = read.csv('ShellStockPrice.csv', header=TRUE)
CompBP = read.csv('BPStockPrices.csv', header=TRUE)
CompChev = read.csv('ChevronStockPrices.csv', header=TRUE)
merge <- merge(CompEx ,CompShell, by=("Date"))
merge <-  merge(merge ,CompBP, by=("Date"))
merge <-  merge(merge ,CompChev, by=("Date"))
colnames(merge) <- c("Date", "Exxon", "Shell", "BP", "Chevron")
merge.returns <- diff(log(as.matrix(merge[, -1]))) * 100
colnames(merge.returns) <- c("BP.returns", "Chevron.returns", "Exxon.returns", "Shell.returns")
# Create size and direction
merge.size <- na.omit(abs(merge.returns)) # size is indicator of volatility
colnames(merge.size) <- c("BP.size", "Chevron.size", "Exxon.size", "Shell.size")
#head(size)
direction <- ifelse(merge.returns > 0, 1, ifelse(merge.returns < 0, -1, 0)) # another indicator of volatility
colnames(direction) <- c("BP.dir", "Chevron.dir", "Exxon.dir", "Shell.dir")
dates <- as.Date(merge$Date[-1], "%m/%d/%Y")
dates.chr <- as.character(merge$Date[-1])
values <- cbind(merge.returns, merge.size, direction)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = merge.returns, size = merge.size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = merge.returns, size = merge.size, direction = direction, stringsAsFactors = FALSE)
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts
# functions
VaR.hist <- function(alpha, data){
# Value at Risk using historical simulation
data <- na.omit(data)
data <- na.
}
VaR.norm <- function(alpha,mu,sigma,S=1){
# The VaR under the assumption of a normal-distribution
- S * ( mu + qnorm(alpha) * sigma )
}
ES.norm <-  function(alpha,mu,sigma,S=1){
# The ES under the assumption of a normal-distribution
S * ( - mu + sigma * ( dnorm(qnorm(alpha))/alpha ) )
}
VaR.t <-  function(alpha,mu,lambda,nu,S=1){
# The VaR under the assumption of a t-distribution
- S * ( mu + qt(alpha,nu) * lambda )
}
ES.t <-  function(alpha,mu,lambda,nu,S=1){
# The ES under the assumption of a t-distribution
S * ( - mu + lambda * ( dt(qt(alpha,nu),nu) / alpha ) * ( ( nu + qt(alpha,nu)^2 )/( nu - 1 ) ) )
}
VaR.gpd <-  function(alpha,mu,lambda,nu,S=1){
# The VaR under the assumption of a t-distribution
- S * ( mu + qt(alpha,nu) * lambda )
}
ES.gpd <-  function(alpha,mu,lambda,nu,S=1){
# The ES under the assumption of a t-distribution
S * ( - mu + lambda * ( dt(qt(alpha,nu),nu) / alpha ) * ( ( nu + qt(alpha,nu)^2 )/( nu - 1 ) ) )
}
nsim <- 1000
alpha <- 0.01
data <- returns
risk.boot.t <- function(data, ind, alpha = 0.01, nsim = 1000){
# Fit a t-distribution to this data and estimate VaR and ES:
#
data <-  unique(data[ind]) # duplicate values seem to give us trouble
params <-  fitdistr(data, densfun="t", start=list(m=-0.001, s=0.02, df=4.00))
params <-  as.vector(params$estimate)
mu <-  params[1]
sigma <-  params[2] # estimate of sigma = sqrt( (nu-2)/nu ) standard_deviation
nu <-  params[3]
VaR <-  VaR.t(alpha, mu, sigma, nu, nsim)
ES <-  ES.t(alpha, mu, sigma, nu, nsim)
return(c( mu, sigma, nu, VaR, ES ))
}
risk.boot.gpd <-  function(data, ind, alpha = 0.01, qu = .9, nsim = 1000){
# Fit a t-distribution to this data and estimate VaR and ES:
#
require(QRM)
data <-  unique(data[ind]) # duplicate values seem to give us trouble
u <- quantile(data, probs = qu, names = FALSE)
params <-  fit.GPD(data, threshold = u, start=list(m=-0.001, s=0.02, df=4.00))
params <-  as.vector(params$estimate)
mu <-  params[1]
sigma <-  params[2] # estimate of sigma = sqrt( (nu-2)/nu ) standard_deviation
nu <-  params[3]
VaR <-  VaR.t(alpha, mu, sigma, nu, nsim)
ES <-  ES.t(alpha, mu, sigma, nu, nsim)
return(c( mu, sigma, nu, VaR, ES ))
}
# Chunk 3
renderPlotly({
library(ggplot2)
#library(ggfortify)
library(plotly)
#title.chg1 <- "Metals Price Percent Changes"
#title.chg2 <- "Size of Metals Price Percent Changes"
p <- autoplot.zoo(data.xts[,1:4]) # + ggtitle(title.chg1) #+ ylim(-5, 5)
ggplotly(p)
})
# Chunk 4
renderPlotly({
#title.chg1 <- "metals Price Percent Changes"
#title.chg2 <- "Size of metals Price Percent Changes"
p <- autoplot.zoo(abs(data.xts[,1:4])) # + ggtitle(title.chg2) #+ ylim(-5, 5)
ggplotly(p)
})
# Chunk 5
renderPlot({
acf(coredata(data.xts[,1:4])) # returns
})
# Chunk 6
renderPlot({
acf(coredata(data.xts[,5:8])) # sizes
})
# Chunk 7
sliderInput("alpha.q", label = "Risk Measure quantiles (%):",
min = 0.75, max = 0.99, value = 0.75, step = 0.01)
# Chunk 8
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included
renderValueBox({
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
returns1 <- returns[,1]
colnames(returns1) <- "Returns" #kluge to coerce column name for df below
q <- quantile(returns1,alpha())
VaR.hist <- q
valueBox(round(VaR.hist, 2),
icon = "glyphicon-signal")
})
# Chunk 9
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included
renderValueBox({
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
returns1 <- returns[,2]
colnames(returns1) <- "Returns" #kluge to coerce column name for df below
q <- quantile(returns1,alpha())
VaR.hist <- q
valueBox(round(VaR.hist, 2),
icon = "glyphicon-signal")
})
# Chunk 10
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included
renderValueBox({
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
returns1 <- returns[,3]
colnames(returns1) <- "Returns" #kluge to coerce column name for df below
q <- quantile(returns1,alpha())
VaR.hist <- q
valueBox(round(VaR.hist, 2),
icon = "glyphicon-signal")
})
# Chunk 11
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included
renderValueBox({
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
returns1 <- returns[,4]
colnames(returns1) <- "Returns" #kluge to coerce column name for df below
q <- quantile(returns1,alpha())
VaR.hist <- q
valueBox(round(VaR.hist, 2),
icon = "glyphicon-signal")
})
# Chunk 12
renderPlot({
returns1 <- returns[,1]
colnames(returns1) <- "Returns" #kluge to coerce column name for df
returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
# Value at Risk
VaR.hist <- quantile(returns1,alpha())
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
# Determine the max y value of the desity plot.
# This will be used to place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
# Expected Shortfall
ES.hist <- median(returns1[returns1 > VaR.hist])
ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
ggplot(returns1.df, aes(x = Returns, fill = Distribution)) +
geom_density(alpha = 0.8) +
geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
annotate("text", x = VaR.hist, y = VaR.y*1.05, label = VaR.text) +
annotate("text", x = ES.hist, y = VaR.y*1.1, label = ES.text)
})
# Chunk 13
renderPlot({
returns1 <- returns[,2]
colnames(returns1) <- "Returns" #kluge to coerce column name for df
returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
# Value at Risk
VaR.hist <- quantile(returns1,alpha())
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
# Determine the max y value of the desity plot.
# This will be used to place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
# Expected Shortfall
ES.hist <- median(returns1[returns1 > VaR.hist])
ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
ggplot(returns1.df, aes(x = Returns, fill = Distribution)) +
geom_density(alpha = 0.8) +
geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
annotate("text", x = VaR.hist, y = VaR.y*1.05, label = VaR.text) +
annotate("text", x = ES.hist, y = VaR.y*1.1, label = ES.text)
})
# Chunk 14
renderPlot({
returns1 <- returns[,3]
colnames(returns1) <- "Returns" #kluge to coerce column name for df
returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
# Value at Risk
VaR.hist <- quantile(returns1,alpha())
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
# Determine the max y value of the desity plot.
# This will be used to place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
# Expected Shortfall
ES.hist <- median(returns1[returns1 > VaR.hist])
ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
ggplot(returns1.df, aes(x = Returns, fill = Distribution)) +
geom_density(alpha = 0.8) +
geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
annotate("text", x = VaR.hist, y = VaR.y*1.05, label = VaR.text) +
annotate("text", x = ES.hist, y = VaR.y*1.1, label = ES.text)
})
# Chunk 15
renderPlot({
returns1 <- returns[,4]
colnames(returns1) <- "Returns" #kluge to coerce column name for df
returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
# Value at Risk
VaR.hist <- quantile(returns1,alpha())
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
# Determine the max y value of the desity plot.
# This will be used to place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
# Expected Shortfall
ES.hist <- median(returns1[returns1 > VaR.hist])
ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
ggplot(returns1.df, aes(x = Returns, fill = Distribution)) +
geom_density(alpha = 0.8) +
geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
annotate("text", x = VaR.hist, y = VaR.y*1.05, label = VaR.text) +
annotate("text", x = ES.hist, y = VaR.y*1.1, label = ES.text)
})
# Chunk 16
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
library(moments)
library(matrixStats)
mean.r <- colMeans(data)
median.r <- colMedians(data)
sd.r <- colSds(data)
IQR.r <- colIQRs(data)
skewness.r <- skewness(data)
kurtosis.r <- kurtosis(data)
result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 1:4])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
# Chunk 17
######################################
corr.rolling <- function(x) {
dim <- ncol(x)
corr.r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
return(corr.r)
}
require(zoo)# PAGE: Market risk
corr.rolling <- function(x) {
dim <- ncol(x)
corr.r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
return(corr.r)
}
ALL.r <- data.xts[, 1:3]
window <- 90 #reactive({input$window})
corr.returns <- rollapply(ALL.r, width = window, corr.rolling, align = "right", by.column = FALSE)
corr.returns.df <- data.frame(Date = index(corr.returns), Bp.Chevron = corr.returns[,1], Bp.Exxon = corr.returns[,2], Chevron.Exxon = corr.returns[,3])
# Market dependencies
library(matrixStats)
R.corr <- apply.monthly(as.xts(ALL.r), FUN = cor)
R.vols <- apply.monthly(ALL.r, FUN = colSds) # from MatrixStats
# Form correlation matrix for one month
R.corr.1 <- matrix(R.corr[20,], nrow = 3, ncol = 3, byrow = FALSE)
rownames(R.corr.1) <- colnames(ALL.r[,1:3])
colnames(R.corr.1) <- rownames(R.corr.1)
R.corr <- R.corr[, c(2, 3, 6)]
colnames(R.corr) <- colnames(corr.returns)
colnames(R.vols) <- c("BP.vols", "Chevron.vols", "Exxon.vols")
R.corr.vols <- na.omit(merge(R.corr, R.vols))
BP.vols <- as.numeric(R.corr.vols[,"BP.vols"])
Chevron.vols <- as.numeric(R.corr.vols[,"Chevron.vols"])
Exxon.vols <- as.numeric(R.corr.vols[,"Exxon.vols"])
library(quantreg)
# hist(rho.fisher[, 1])
bp.corrs <- R.corr.vols[,1]
#hist(nickel.corrs)
taus <- seq(.05,.95,.05)	# Roger Koenker UI Bob Hogg and Allen Craig
fit.rq.bp.chevron <- rq(bp.corrs ~ Chevron.vols, tau = taus)
fit.lm.bp.chevron <- lm(bp.corrs ~ Chevron.vols)
# Chunk 18
#library(psych)
pairs.panels(corr.returns.df)
# Chunk 19
renderPlotly({
p <- ggplot(corr.returns.df, aes(x = Date, y = Bp.Chevron)) + geom_line()
ggplotly(p)
})
# Chunk 20
renderPlotly({
p <- ggplot(corr.returns.df, aes(x = Date, y = Bp.Exxon)) + geom_line()
ggplotly(p)
})
# Chunk 21
renderPlotly({
p <- ggplot(corr.returns.df, aes(x = Date, y = Chevron.Exxon)) + geom_line()
ggplotly(p)
})
# Chunk 22
plot.zoo(R.corr.vols, main= "Monthly Correlations and Volatilities")
# Chunk 23
renderPlot({
plot(summary(fit.rq.bp.chevron), parm = "Chevron.vols", main = "BP Chevron correlation sensitivity to Chevron volatility")
})
# Chunk 24
## Now for Loss Analysis
# Get last prices
price.last <- as.numeric(tail(merge[, -1], n=1))
# Specify the positions
position.rf <- c(1/4,1/4,1/4,1/4)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(merge.returns), ncol=ncol(merge.returns), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(merge.returns/100) * weights.rf)
loss.rf.df <- data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))
## Simple Value at Risk and Expected Shortfall
alpha.tolerance <- .95
VaR.hist <- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)
## Just as simple Expected shortfall
ES.hist <- median(loss.rf[loss.rf > VaR.hist])
VaR.text <- paste("Value at Risk =\n", round(VaR.hist, 2)) # ="VaR"&c12
ES.text <- paste("Expected Shortfall \n=", round(ES.hist, 2))
title.text <- paste(round(alpha.tolerance*100, 0), "% Loss Limits")
renderPlotly({
p <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_histogram(alpha = 0.8) + geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") + geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") + annotate("text", x = VaR.hist, y = 40, label = VaR.text) + annotate("text", x = ES.hist, y = 20, label = ES.text) + xlim(0, 50) + ggtitle(title.text)
ggplotly(p)
})
# Chunk 25
library(quantreg)
x <- merge.returns/100
n <- nrow(x)
p <- ncol(x)
alpha <-  c(0.1, 0.3) # quantiles
w <-  c(0.3, 0.7) # distortion weights
lambda <- 100 # Lagrange multiplier for adding up constraint
m <- length(alpha)
# error handling: if (length(w) != m) stop("length of w doesn't match length of alpha")
xbar <- apply(x, 2, mean)
mu.0 <-  mean(xbar)
y <- x[, 1] #set numeraire
r <- c(lambda * (xbar[1] - mu.0), -lambda * (xbar[1] - mu.0))
X <- x[, 1] - x[, -1]
R <- rbind(lambda * (xbar[1] - xbar[-1]), -lambda * (xbar[1] - xbar[-1]))
R <- cbind(matrix(0, nrow(R), m), R)
f <- rq.fit.hogg(X, y, taus = alpha, weights = w, R = R, r = r)
fit <- f$coefficients
# transform regression coeff to portfolio weights
pihat <- c(1 - sum(fit[-(1:m)]), fit[-(1:m)])
x <- as.matrix(x)
yhat <- x %*% pihat # predicted
etahat <- quantile(yhat, alpha)
muhat <- mean(yhat)
qrisk <- 0
for (i in 1:length(alpha)) qrisk <- qrisk + w[i] * sum(yhat[yhat < etahat[i]])/(n * alpha[i])
qrisk
pihat
# Chunk 26
mu.0 <- xbar
mu.P <- seq(-.0005, 0.0015, length = 100) ## set of 300 possible target portfolio returns
qrisk.P <-  mu.P ## set up storage for quantile risks of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(merge.returns)) ## storage for portfolio weights
colnames(weights) <- names(merge.returns)
for (i in 1:length(mu.P))
{
mu.0 <-  mu.P[i]  ## target returns
result <- qrisk(x, mu = mu.0)
qrisk.P[i] <- -result$qrisk # convert to loss risk already weighted across alphas
weights[i,] <-  result$pihat
}
qrisk.mu.df <- data.frame(qrisk.P = qrisk.P, mu.P = mu.P )
mu.P <- qrisk.mu.df$mu.P
mu.free <-  0.0003 ## input value of risk-free interest rate
sharpe <- ( mu.P-mu.free)/qrisk.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (qrisk.P == min(qrisk.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## find the efficient frontier (blue)
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
weights.extr <- weights[ind,] # for use in calculating tengency risk measures
qrisk.mu.df$col.P <- col.P
renderPlotly({
p <- ggplot(qrisk.mu.df, aes(x = qrisk.P, y = mu.P, group = 1)) + geom_line(aes(colour= col.P, group = col.P)) + scale_colour_identity()
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=3)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/qrisk.P[ind], colour = "red")
p <- p + geom_point(aes(x = qrisk.P[ind], y = mu.P[ind]))
p <- p + geom_point(aes(x = qrisk.P[ind2], y = mu.P[ind2]))
ggplotly(p)
})
# Chunk 27
library(quadprog)
R <- returns[,1:4]/100
quantile_R <- quantile(R[,1], 0.95)
#R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
names.R <- colnames(R)
mean.R <-  apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R)) ## remember these are in daily percentages
#library(quadprog)
Amat <-  cbind(rep(1,4),mean.R)  ## set the equality constraints matrix
mu.P <- seq(0.5*min(mean.R), 1.5*max(mean.R), length = 100)  ## set of 300 possible target portfolio returns
#mu.P <- seq(0.5*quantile_R, max(R), length = 100)  ## set of 300 possible target portfolio returns
sigma.P <-  mu.P ## set up storage for std dev's of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights
colnames(weights) <- names.R
for (i in 1:length(mu.P))
{
bvec <- c(1,mu.P[i])  ## constraint vector
result <- solve.QP(Dmat=2*cov.R,dvec=rep(0,4),Amat=Amat,bvec=bvec,meq=2)
sigma.P[i] <- sqrt(result$value)
weights[i,] <- result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
mu.free <-  .0003 ## input value of daily risk-free interest rate
sharpe <- ( mu.P-mu.free)/sigma.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P
renderPlotly({
p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) +
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind]))
p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3]) + annotate("text", x = sd.R[4], y = mean.R[4], label = names.R[4])
p	### ggplotly(p)
})
# Chunk 28
weights[ind2,]
# Chunk 29
weights[ind,]
