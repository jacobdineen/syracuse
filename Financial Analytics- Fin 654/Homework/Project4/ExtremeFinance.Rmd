---
title: "Heavy Metal"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    source_code: embed
    theme: "spacelab"
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(plotly)
#library(ggfortify)
library(psych)

rm(list = ls())
# PAGE: Exploratory Analysis
data <- na.omit(read.csv("metaldata.csv", header = TRUE))
# Compute log differences percent using as.matrix to force numeric type
data.r <- diff(log(as.matrix(data[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility
#head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of volatility
colnames(direction) <- paste(colnames(direction),".dir", sep = "")
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(data$DATE[-1], "%m/%d/%Y")
dates.chr <- as.character(data$DATE[-1])
str(dates.chr)
values <- cbind(data.r, size, direction)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE) 
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts

# PAGE: Market risk 
corr.rolling <- function(x) {	
  dim <- ncol(x)	
  corr.r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr.r)	
}

ALL.r <- data.xts[, 1:3]
window <- 90 #reactive({input$window})
corr.returns <- rollapply(ALL.r, width = window, corr.rolling, align = "right", by.column = FALSE)
#colnames(corr.returns) <- c("nickel & copper", "nickel & aluminium", "copper & aluminium")
corr.returns.df <- data.frame(Date = index(corr.returns), nickel.copper = corr.returns[,1], nickel.aluminium = corr.returns[,2], copper.aluminium = corr.returns[,3])

# Market dependencies
library(matrixStats)
R.corr <- apply.monthly(as.xts(ALL.r), FUN = cor)
R.vols <- apply.monthly(ALL.r, FUN = colSds) # from MatrixStats	
# Form correlation matrix for one month 	
R.corr.1 <- matrix(R.corr[20,], nrow = 3, ncol = 3, byrow = FALSE)	
rownames(R.corr.1) <- colnames(ALL.r[,1:3])	
colnames(R.corr.1) <- rownames(R.corr.1)	
R.corr <- R.corr[, c(2, 3, 6)]
colnames(R.corr) <- colnames(corr.returns) 	
colnames(R.vols) <- c("nickel.vols", "copper.vols", "aluminium.vols")	
R.corr.vols <- na.omit(merge(R.corr, R.vols))
nickel.vols <- as.numeric(R.corr.vols[,"nickel.vols"])	
copper.vols <- as.numeric(R.corr.vols[,"copper.vols"])	
aluminium.vols <- as.numeric(R.corr.vols[,"aluminium.vols"])
library(quantreg)
# hist(rho.fisher[, 1])
nickel.corrs <- R.corr.vols[,1]
#hist(nickel.corrs)
taus <- seq(.05,.95,.05)	# Roger Koenker UI Bob Hogg and Allen Craig
fit.rq.nickel.copper <- rq(nickel.corrs ~ copper.vols, tau = taus)	
fit.lm.nickel.copper <- lm(nickel.corrs ~ copper.vols)	
#' Some test statements	
#summary(fit.rq.nickel.copper, se = "boot")
#'
#summary(fit.lm.nickel.copper, se = "boot")
#plot(summary(fit.rq.nickel.copper), parm = "copper.vols", main = "nickel-copper correlation sensitivity to copper volatility") #, ylim = c(-0.1 , 0.1))	
```


Decision
=======================================================================

Background {.tabset}
-----------------------------------------------------------------------
### Introduction

A freight forwarder with a fleet of bulk carriers wants to optimize their portfolio in the metals markets with entry into the nickel business and use of the tramp trade.  Tramp ships are the company's "swing" option without any fixed charter or other constraint. They allow the company flexibility in managing several aspects of freight uncertainty.   They have allocated \$250 million to purchase metals. The company wants us to:

1.	Retrieve and begin to analyze data about potential commodities to diversify into
2.	Compare potential commodities with existing commodities in conventional metals spot markets
3.	Begin to generate economic scenarios based on events that may, or may not, materialize in the commodities
4.	The company wants to mitigate their risk by diversifying their cargo loads

### Decision  

Identify the optimal combination of Nickel, Copper, and Aluminium to trade

1.	Product: Metals commodities and freight charters
2.	Metal, Company, and Geography:
    a. Nickel: MMC Norilisk, Russia
    b. Copper: Codelco, Chile and MMC Norilisk, Russia
    c. Aluminium: Vale, Brasil and Rio Tinto Alcan, Australia
3.	Customers: Ship Owners, manufacturers, traders
4.  All metals traded on the London Metal Exchange 

### Business questions

1.	How would the performance of these commodities affect the size and timing of shipping arrangements?
2.	How would the value of new shipping arrangements affect the value of our business with our current customers?
3.	How would we manage the allocation of existing resources given we have just landed in this new market?


Approach {.tabset}
-----------------------------------------------------------------------

### Stylized facts of the Metals market

The London Metal Exchange (LME) is the world's centre for commodity exchange and the majority of non-ferrous metal is conducted on its' market.  

- In 2016 the LME traded $10.3T USD notional, which included the exchange of 3.5B m/t
- Volatility is rarely constant and often has a structure (mean reversion) and is dependent on the past.
- Past shocks persist and may or may not dampen (rock in a pool).
- Extreme events are likely to happen with other extreme events.
- Negative returns are more likely than positive returns (left skew).

### History speaks

- We will develop the *value at risk* and *expected shortfall* metrics from the historical simulated distributions of risk factors.
- Given these factors we will combine them into a portfolio and calculate their losses. 
- With the loss distribution in hand we can compute the risk measures. - This approach is nonparametric.

- We can then posit high quantile thresholds and explore risk measures the in the tails of the distributions.

First we set the tolerance level $\Large\alpha$, for example, equal to 95\%. This would mean that a decision maker would not tolerate loss in  more than $\Large 1-\alpha$, or 5\%. of all risk scenarios under consideration.

We define the VaR as the quantile for probability $\Large\alpha \in (0,1)$, as

$$
\Large
VaR_{\alpha} (X) = inf \{ x \in R: F(x) \geq \alpha \},
$$

which means find the greatest lower bound of loss $\Large x$ (what the symbol $\Large inf$ = _infimum_ means in English), such that the cumulative probability of $\Large x$ is greater than or equal to $\Large \alpha$. 

Using the $\Large VaR_{\alpha}$ definition we can also define $\Large ES$ as

$$
\Large
ES_{\alpha} = E [X \lvert X \geq VaR_{\alpha}],
$$

where $\Large ES$ is "expected shortfall" and $\Large E$ is the expectation operator, also known as the "mean." Again, in English, the expected shortfall is the average of all losses greater than the loss at a $\Large VaR$ associated with probability $\Large \alpha$, and $\Large ES \geq VaR$.

### Data and analysis to inform the decision

- Spot market prices of nickel, copper, and aluminium
- Nickel and Copper: correlation
- Nickel and Aluminium: correlation
- Copper and Aluminium: Correlation
- Nickel and Copper: Correlation sensitivity to copper dependency
- All together: correlations and volaitlities among these indicators
- Cross-section of rolling correlation will be visualize correlation

Data
=======================================================================

Column
-----------------------------------------------------------------------

### Data Definitions

- *Nickel*: daily nickel price (\$/per metric ton)
- *Copper*: daily copper prices (\$/per metric ton)
- *Aluminium *: daily aluminium prices (\$/per metric ton)

### metals Price Percent Changes
```{r}
renderPlotly({
  library(ggplot2)
  #library(ggfortify)
  library(plotly)
  #title.chg1 <- "Metals Price Percent Changes"
  #title.chg2 <- "Size of Metals Price Percent Changes"
  p <- autoplot.zoo(data.xts[,1:3]) # + ggtitle(title.chg1) #+ ylim(-5, 5)
  ggplotly(p)
})
```

Row 
-----------------------------------------------------------------------

### Initial Analysis of Nickel, Copper, Aluminium 

Historical data 2012-2016

- Nickel has experienced a number of spikes in price and magnitude percentage change.  
- Copper is less volatile in terms of price and magnitude percentage change
- Aluminium experienced some shocks of volatility in 2015 and 2016

### Size of Metals Price Percent Changes

```{r}
renderPlotly({
  #title.chg1 <- "metals Price Percent Changes"
  #title.chg2 <- "Size of metals Price Percent Changes"
  p <- autoplot.zoo(abs(data.xts[,1:3])) # + ggtitle(title.chg2) #+ ylim(-5, 5)
  ggplotly(p)
  })
```



Exploratory Analysis
=======================================================================

Row {.sidebar}
-----------------------------------------------------------------------
A quantile divides the returns distribution into two groups. For example 75\% of all returns may fall below a return value of 10\%. The distribution is thus divided into returns above 10\% and below 10\% at the 75\% quantile.

Pull slide to the right to measure the risk of returns at desired quantile levels. The minimum risk quantile is 75\%. The maximum risk quantile is 99\%.


```{r}
sliderInput("alpha.q", label = "Risk Measure quantiles (%):",
            min = 0.75, max = 0.99, value = 0.75, step = 0.01)
```


Row
-----------------------------------------------------------------------

### Nickel Value at Risk

```{r}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  returns1 <- returns[,1]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha())
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```

### Copper Value at Risk

```{r}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  returns1 <- returns[,2]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha())
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```

### Aluminium Value at Risk

```{r}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  returns1 <- returns[,3]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha())
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```


Row {.tabset .tabset-fade}
-----------------------------------------------------------------------

### nickel metals Returns Distribution

```{r}
renderPlotly({
  returns1 <- returns[,1]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df
  returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
  
  alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  
  # Value at Risk
  VaR.hist <- quantile(returns1,alpha())
  VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR.y <- max(density(returns1.df$Returns)$y)
  
  # Expected Shortfall
  ES.hist <- median(returns1[returns1 > VaR.hist])
  ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
  
  p <- ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+ VaR.hist, y = VaR.y*1.05, label = VaR.text) +
    annotate("text", x = 1.5+ ES.hist, y = VaR.y*1.1, label = ES.text) +     scale_fill_manual( values = "dodgerblue4")
  ggplotly(p)
  })
```

###  Copper Returns Distribution

```{r}
renderPlotly({
  returns1 <- returns[,2]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df
  returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
  
alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  
  # Value at Risk
  VaR.hist <- quantile(returns1,alpha())
  VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR.y <- max(density(returns1.df$Returns)$y)
  
  # Expected Shortfall
  ES.hist <- median(returns1[returns1 > VaR.hist])
  ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
  
  p <- ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+VaR.hist, y = VaR.y*1.05, label = VaR.text) +
    annotate("text", x = 1.5+ES.hist, y = VaR.y*1.1, label = ES.text)+     scale_fill_manual( values = "dodgerblue4")
  ggplotly(p)
})
```

### Aluminium Returns Distribution

```{r}
renderPlotly({
  returns1 <- returns[,3]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df
  returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
  ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + geom_density(alpha = 0.8)
  
  alpha <- reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  
  # Value at Risk
  VaR.hist <- quantile(returns1,alpha())
  VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR.y <- max(density(returns1.df$Returns)$y)
  
  # Expected Shortfall
  ES.hist <- median(returns1[returns1 > VaR.hist])
  ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
  
  p <- ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+VaR.hist, y = VaR.y*1.05, label = VaR.text) +
    annotate("text", x = 1.5+ES.hist, y = VaR.y*1.1, label = ES.text) +     scale_fill_manual( values = "dodgerblue4")
  ggplotly(p)
})

```

### 
```{r }
renderPlotly({
  acf(coredata(data.xts[,1:3])) # returns
})
```

```{r}
renderPlotly({
  acf(coredata(data.xts[,4:5])) # sizes
})
```

### Statistics
```{r}
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 1:3])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
```


Market Risk
=======================================================================

Row {.tabset}
-----------------------------------------------------------------------

### Nickel, Copper, Aluminium outliers
- This scatterplot illustrates some of the outliers of each of the three metals.  Aluminium appears to be more concentrated around the mean but also experienced its share of volatility
- Copper is also closely grouped around its mean
- Nickel experienced the most volatility with points appearing the furthest from the mean.

### Nickel, Copper, Aluminium

```{r}
renderPlotly({
  p <- autoplot(ALL.r, ts.geom= 'point' , facets = FALSE)
  ggplotly(p)
  })
```

###Correlation return drill-down

- There is a strong correlation (.89) between nickel and copper.  This was initially expected since both of these metals are used in similar applications or in the creation of cupronickel alloy which is used for desalinisation due its high resistant to corrosion.  

- Cupronickel is also used in a variety of other applications ranging from minting, armaments, marine engineering, electrical applications, and for the repair of fan blades found in geothermal power plants.  

### Nickel, Copper, Aluminium relationships

```{r}
#library(psych)
pairs.panels(corr.returns.df)
```


row {.tabset }
-----------------------------------------------------------------------

### nickel and copper (90 day rolling correlation)

```{r }
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = nickel.copper)) + geom_line()
  ggplotly(p)
})
```

### nickel and aluminium (90 day rolling correlation)

```{r }
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = nickel.aluminium)) + geom_line()
  ggplotly(p)
})
```

### copper and aluminium (90 day rolling correlation)

```{r }
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = copper.aluminium)) + geom_line()
  ggplotly(p)
})
```

### 30 day within-sample correlations and volatilities

```{r}
plot.zoo(R.corr.vols, main= "Monthly Correlations and Volatilities")
```

### nickel - copper Dependency

```{r}
renderPlot({ 
  plot(summary(fit.rq.nickel.copper), parm = "copper.vols", main = "nickel-copper correlation sensitivity to copper volatility")
})
```


Loss measurement
=======================================================================

row {.tabset}
-----------------------------------------------------------------------

### Measuring Loss

- Loss, $\Large L_t$, derives from negative $\Large P\&L$ and erupts in a negative change in the value of $\Large P\&L$, $\Large V_t$ over time, $\Large -\Delta V_t$. Suppose we have $\Large d$ lines of business and random variable losses $\Large L_1,\dots,L_d$ for each line defined on a common probability space $\Large (\Omega,\mathcal{F},P)$.

- Each line of business $\Large i = 1, \dots, d$ has weight $\Large \lambda \in \Lambda \subset \mathbb{R}^d \backslash \{\mathbb{0}\}$ so that the portfolio loss is 

$$\Large
L(\lambda)=\sum_{i=1}^d \lambda_iL_i$$

- Let $\Large \mathcal{\rho}$ be a risk measure, and define an associated risk measure $\Large r_Q$ as a function $\Large r_Q: \Lambda \rightarrow \mathbb{R}$ so that $\Large r_Q(\lambda) = r_Q(\rho(\lambda))$ on the portfolio of positions associated with each line of business.

### Euler Allocation

- If $\Large r_Q$ is positive homogeneous and differentiable at $\Large \lambda \in \Lambda$ then
$$\Large
r_Q(\lambda) = \sum_{i=1}^d \lambda_i \frac{\partial r_Q}{\partial \lambda_i}(\lambda)$$

- At $\Large \lambda = 1$, so that the positions are portfolio weights, then the *Euler capital allocation principle* is for $\Large 1 \leq i \leq d$
$$\Large
C_i = \frac{\partial r_Q}{\partial \lambda_i}(1)$$

- If $\Large r_{SD}= \sqrt{var(L(\lambda))}$, and $\Large \Sigma$ is the variance-covariance matrix of the losses of the lines of business $\Large L_1,\dots,L_d$, then

$$
\Large
r_{SD}(\lambda) = \sqrt{\lambda^T \Sigma \lambda}
$$
We then have allocated capital for each line of business
$$
\Large
C_{Q,i}=\frac{\partial r_{SD}}{\partial \lambda_i}(1) = \frac{(\Sigma 1)_i}{r_{SD}(1)}=\frac{\Sigma_{j=1}^d cov(L_j, L_i)}{r_{SD}(1)}=\frac{cov(L_i,L)}{\sqrt{var(L)}}
$$

### Value at risk and expected shortfall

- Define a risk measure called value at risk, $\Large VaR$ as the quantile $\Large q$ of loss $\Large L$ at a tolerance level of $\Large \alpha$, so that, $\Large r_{VaR}^{\alpha}= q_{\alpha}(L(\lambda))$.
- Tasche (2000) shows that, for the lines of business $\Large L_1,\dots,L_d$, here is a value at risk allocation for each line of business
$$\Large
C_{Q,i}=\frac{\partial r_{SD}}{\partial \lambda_i}(1) = E(L_i \, | \, L = q_{\alpha}(L))$$

- Apply the Euler allocation principle to calculate the expected shortfall $\Large ES$ as

$$
\Large
\frac{\partial r_{ES}^{\alpha}}{\partial \lambda_i}(1) = \frac{1}{1-\alpha}\int_{\alpha}^1 \frac{\partial r_{VaR}^{\alpha}}{\partial \lambda_i}(1)du = \frac{1}{1-\alpha}\int_{\alpha}^1E(L_i \, | \, L = q_{u}(L))du
$$

- Assume that the loss density $\Large f_L$ is strictly positive so that the distribution function of loss possesses a diffentiable inverse and change variables so that $\Large v = q_u(L) = F_L(u)$ the cumulative loss distribution. Then 

$$
\Large
\frac{dv}{du} = f^{-1}(v)
$$
and we can compute
$$
\Large
\frac{\partial r_{ES}^{\alpha}}{\partial \lambda_i}(1) = \frac{1}{1-\alpha}\int_{q_{\alpha}(L)}^{\infty}E(L_i | L=v)f_L(v)dv = \frac{1}{1-\alpha}\int_{\alpha}^1E(L_i \, | \, L \geq q_{\alpha}(L))
$$
- (Finally) we have the expected shortfall contribution of a line of business $\Large i$ as
$$
\Large
C_i^{ES} = E(L_i | L \geq VaR_{\alpha}(L))
$$

### Getting practical

- Using price returns we can compute loss. 
- Weights for each are defined as the value of the positions in each risk factor. 
- We can compute this as the notional (in tonnes equivalent for this market) times the last observed price.
- Losses for a barrel equivalent of the three commodities are computed back into the sample relative to the most recently observed prices

### Empirical loss

```{r }
# Get last prices
price.last <- as.numeric(tail(data[, -1], n=1))
# Specify the positions
position.rf <- c(1/3, 1/3, 1/3)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(data.r/100) * weights.rf)
loss.rf.df <- data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))
## Simple Value at Risk and Expected Shortfall
alpha.tolerance <- .95
VaR.hist <- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)
## Just as simple Expected shortfall
ES.hist <- median(loss.rf[loss.rf > VaR.hist])
VaR.text <- paste("Value at Risk =\n", round(VaR.hist, 2)) # ="VaR"&c12
ES.text <- paste("Expected Shortfall \n=", round(ES.hist, 2))
title.text <- paste(round(alpha.tolerance*100, 0), "% Loss Limits")
renderPlotly({
  p <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_histogram(alpha = 0.8) + geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") + geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") + annotate("text", x = VaR.hist, y = 40, label = VaR.text) + annotate("text", x = ES.hist, y = 20, label = ES.text) + xlim(0, 500) + ggtitle(title.text)
  ggplotly(p)
})
```

Extremes
=======================================================================
Row {.tabset}
-----------------------------------------------------------------------
### Let's go to extremes

- All along we have been stylizing financial returns, commodities and exchange rates, as skewed and with thick tails.
- We next go on to an extreme tail distribution called the Generalized Pareto Distribution (GPD). 
- For very high thresholds, GPD not only well describes behavior in excess of the threshold, but the mean excess over the threshold is linear in the threshold. 
- From this we get more intuition around the use of expected shortfall as a coherent risk measure. 
- In recent years we well exceeded all Gaussian and Student's t thresholds.

For a random variate $\Large x$, this distribution is defined for the shape parameters $\Large \xi \geq 0$ as:

$$
\Large
g(x; \xi \geq 0) = 1- (1 + x \xi/\beta)^{-1/\xi}
$$


and when the shape parameter $\Large \xi = 0$, the GPD becomes the exponential distribution dependent only on the scale parameter $\beta$:

$$
\Large
g(x; \xi = 0) = 1 - exp(-x/\beta).
$$

Now for one reason for GPD's notoriety...

- If $\Large u$ is an upper (very high) threshold, then the excess of threshold function for the GPD is

$$
\Large
e(u) = \frac{\beta + \xi u}{1 - \xi}.
$$

- This simple measure is _linear_ in thresholds. 
- It will allow us to visualize where rare events begin (see McNeil, Embrechts, and Frei (2015, chapter 5)). 
- we often exploit this property when we look at operational loss data.
- Here is a mean excess loss plot for the `loss.rf` data. If there is a straight-line relationship after a threshold, then we have some evidence for the existence of a GPD for the tail.

### Mean excess loss

```{r}
data <- as.vector(loss.rf) # data is purely numeric
umin <-  min(data)         # threshold u min
umax <-  max(data) - 0.1   # threshold u max
nint <- 100                # grid length to generate mean excess plot
grid.0 <- numeric(nint)    # grid store
e <- grid.0                # store mean exceedances e
upper <- grid.0            # store upper confidence interval
lower <- grid.0            # store lower confidence interval
u <- seq(umin, umax, length = nint) # threshold u grid
alpha <- 0.95                  # confidence level
for (i in 1:nint) {
    data <- data[data > u[i]]  # subset data above thresholds
    e[i] <- mean(data - u[i])  # calculate mean excess of threshold
    sdev <- sqrt(var(data))    # standard deviation
    n <- length(data)          # sample size of subsetted data above thresholds
    upper[i] <- e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
    lower[i] <- e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
  }
mep.df <- data.frame(threshold = u, threshold.exceedances = e, lower = lower, upper = upper)
loss.excess <- loss.rf[loss.rf > u]
renderPlotly({
  p <- ggplot(mep.df, aes( x= threshold, y = threshold.exceedances)) + geom_line() + geom_line(aes(x = threshold, y = lower), colour = "red") + geom_line(aes(x = threshold,  y = upper), colour = "red") + annotate("text", x = 400, y = 200, label = "upper 95%") + annotate("text", x = 200, y = 0, label = "lower 5%")
  ggplotly(p)
})
```

### GPD fits and starts

```{r}
#library(QRM)
alpha.tolerance <- 0.95
u <- quantile(loss.rf, alpha.tolerance , names=FALSE)
fit <- fit.GPD(loss.rf, threshold=u) # Fit GPD to the excesses
xi.hat <- fit$par.ests[["xi"]] # fitted xi
beta.hat <- fit$par.ests[["beta"]] # fitted beta
data <- loss.rf
n.relative.excess <- length(loss.excess) / length(loss.rf) # = N_u/n
VaR.gpd <- u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1) 
ES.gpd <- (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)
n.relative.excess <- length(loss.excess) / length(loss.rf) # = N_u/n
VaR.gpd <- u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1) 
ES.gpd <- (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)
# Plot away
renderPlotly({
  VaRgpd.text <- paste("GPD: Value at Risk =", round(VaR.gpd, 2))
  ESgpd.text <- paste("Expected Shortfall =", round(ES.gpd, 2))
  title.text <- paste(VaRgpd.text, ESgpd.text, sep = " ")
  loss.plot <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2)
  loss.plot <- loss.plot + geom_vline(aes(xintercept = VaR.gpd), colour = "blue", linetype = "dashed", size = 0.8)
  loss.plot <- loss.plot + geom_vline(aes(xintercept = ES.gpd), colour = "blue", size = 0.8) 
  #+ annotate("text", x = 300, y = 0.0075, label = VaRgpd.text, colour = "blue") + annotate("text", x = 300, y = 0.005, label = ESgpd.text, colour = "blue")
  loss.plot <- loss.plot + xlim(0,500) + ggtitle(title.text)
  ggplotly(loss.plot)
})
```

### Confidence and risk measures

```{r}
showRM(fit, alpha = 0.99, RM = "ES", method = "BFGS")
```


Optimization
==========================================================
row {.tabset}
----------------------------------------------------------

### If that wasn't enough...

Stylized market facts indicate

- Allocation across various component of loss drivers requires both body and tail considerations
- Pessisimistic risk measurement requires some sort of distortion measure to assess the probability of good and bad news

So that ...

- Bassett et al.(2004) show that the mean-expected shortfall efficient portfolio problem is equivalent to a quantile regression with linear constraints.
- Enlarge scope of expected utility from monetary and probabality to include an assessment (distortion) of probability.
- Choquet integrals build on Lebesgue measures by inflating or deflating the probabilities by the rank order of the outcomes.
- Expected shortfall is an example of a Choquet, rank-ordered, criterion.

### Pessimism reigns

- A risk measure $\Large \rho$ is pessistic if, for some probability measure $\Large \phi$ on $\Large [0,1]$, 
$$
\Large
\rho(L) = \int_0^1 \rho_{u}(L) \phi(u) du.
$$

- For expected shortfall, $\Large \phi(u) = (1-\alpha)^{-1}I_{(u\geq\alpha)}$: equal weight is placed on all quantiles beyond the $\alpha$-quantile.
- Suppose we have a loss portfolio with position weights $\Large \pi$ and losses $X$ so that total loss is $\Large L = X\,'\pi$ with mean loss $\Large \mu(L)$. Let's choose loss weights to minimize
$$
\Large
min_{\pi}\,[\rho_{\alpha}(L) - \lambda \mu(L)] \,\, s.t.\, \mu(L)=\mu_0, \,\, 1^T\pi = 1
$$

where the weights add up to 1 and we try to achieve a minimum return $\Large \mu_0$.

- Taking this formulation to a sample version for $\Large n$ observations of losses, we get
$$
\Large
min_{\beta, \xi}\sum_{k=1}^m \, \sum_{i=1}^n \, \nu_k \rho_{\alpha}(X_{i1}-\sum_{j=2}^p (x_{i1}-x_{ij}\beta_{j})-\xi_k))
$$

$$
\Large
s.t.\, \bar{X}\,'\pi(\beta) = \mu_0
$$

- In this approach, there are $\Large m$ weights $\Large \nu$ that pull together $\Large m$ different sets of portfolio weightings. The $\Large \xi$ terms represent $\Large m$ different intercepts, one for each $\Large \nu_k$ weight. 
- There are $\Large p$ assets or loss categories here. We use the first asset, $\Large i = 1$ as the "numerarire" or benchmark asset. We measure returns on assets 2 to $\Large p$ relative to the first asset. The weights for assets 2 to $\Large p$ are the regression coeffients $\Large \beta$. the weight for the first asset uses the adding up constraint so that

$$
\Large
\pi_1 = 1 - \sum_{j=2}^p \pi_j
$$


The corresponding Markowitz (1952) approach is
$$
\Large
min_{\beta, \xi} \, \sum_{i=1}^n (X_{i1}-\sum_{j=2}^p (x_{i1}-x_{ij})\beta_{j}-\xi))^2
$$
subject to the constraint

$$
\Large
s.t.\, \bar{X}\,'\pi = \mu_0
$$

- We model distortions using weighted quantiles.
- The Choquet criterion ends up using a weighted average of quantile allocations across assessed probabilities to express preferences.
- Mimimize a weighted sum of quantile regression objective functions using the specified $\alpha$ quantiles. 
- The model permits distinct intercept parameters at each of the specified taus, but the slope parameters are constrained to be the same for all $\Large \alpha$s. 
- This estimator was originally suggested to the Roger Koenker by Bob Hogg in one of his famous blue book notes of 1979. 
- The algorithm used to solve the resulting linear programming problems is either the Frisch Newton algorithm described in Portnoy and Koenker (1997), or the closely related algorithm described in Koenker and Ng(2002) that handles linear inequality constraints. 
- Linear inequality constraints can be imposed.

```{r  mysize=TRUE, size='\\footnotesize', echo = TRUE}
library(quantreg)
x <- data.r/100
n <- nrow(x)
p <- ncol(x)
alpha <-  c(0.1, 0.3) # quantiles
w <-  c(0.3, 0.7) # distortion weights
lambda <- 100 # Lagrange multiplier for adding up constraint
m <- length(alpha)
# error handling: if (length(w) != m) stop("length of w doesn't match length of alpha")
xbar <- apply(x, 2, mean)
mu.0 <-  mean(xbar)
y <- x[, 1] #set numeraire
r <- c(lambda * (xbar[1] - mu.0), -lambda * (xbar[1] - mu.0))
X <- x[, 1] - x[, -1]
R <- rbind(lambda * (xbar[1] - xbar[-1]), -lambda * (xbar[1] - xbar[-1]))
R <- cbind(matrix(0, nrow(R), m), R)
f <- rq.fit.hogg(X, y, taus = alpha, weights = w, R = R, r = r)
fit <- f$coefficients
# transform regression coeff to portfolio weights
pihat <- c(1 - sum(fit[-(1:m)]), fit[-(1:m)]) 
x <- as.matrix(x)
yhat <- x %*% pihat # predicted 
etahat <- quantile(yhat, alpha)
muhat <- mean(yhat)
qrisk <- 0
for (i in 1:length(alpha)) qrisk <- qrisk + w[i] * sum(yhat[yhat < etahat[i]])/(n * alpha[i])
qrisk
pihat
```

Trying a different distortion

```{r  mysize=TRUE, size='\\footnotesize', echo = TRUE}
library(quantreg)
#library(dplyr) # use data.df now
alpha <- 0.95
u <- quantile(data.df$returns.nickel, alpha )
x <- data.df.nd[data.df.nd$returns.nickel < u, 2:4]/100
n <- nrow(x)
p <- ncol(x)
alpha <-  c(0.01, 0.1) # quantiles at lower (negative) tail
w <-  c(0.95, 0.05) # distortion weights
lambda <- 100 # Lagrange multiplier for adding up constraint
m <- length(alpha) #alpha and w length must be the same
xbar <- apply(x, 2, mean)
mu.0 <-  mean(xbar)
y <- x[, 1] #set numeraire
r <- c(lambda * (xbar[1] - mu.0), -lambda * (xbar[1] - mu.0))
X <- x[, 1] - x[, -1] # set up design matrix of adjusted all but numeraire returns
R <- rbind(lambda * (xbar[1] - xbar[-1]), -lambda * (xbar[1] - xbar[-1])) # constraints
R <- cbind(matrix(0, nrow(R), m), R) #augmented constraints
f <- rq.fit.hogg(X, y, taus = alpha, weights = w, R = R, r = r) #Bob Hogg estimator
fit <- f$coefficients
# transform regression coeff to portfolio weights
pihat <- c(1 - sum(fit[-(1:m)]), fit[-(1:m)]) 
x <- as.matrix(x)
yhat <- x %*% pihat # predicted 
(etahat <- quantile(yhat, alpha))
(muhat <- mean(yhat))
qrisk <- 0
for (i in 1:length(alpha)) qrisk <- qrisk + w[i] * sum(yhat[yhat < etahat[i]])/(n * alpha[i])
qrisk
pihat
```


### Extreme frontier finance

```{r }
mu.0 <- xbar
mu.P <- seq(-.0005, 0.0015, length = 100) ## set of 300 possible target portfolio returns
qrisk.P <-  mu.P ## set up storage for quantile risks of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(data.r)) ## storage for portfolio weights
colnames(weights) <- names(data.r)
for (i in 1:length(mu.P))
{
  mu.0 <-  mu.P[i]  ## target returns
  result <- qrisk(x, mu = mu.0)
  qrisk.P[i] <- -result$qrisk # convert to loss risk already weighted across alphas
  weights[i,] <-  result$pihat
}
qrisk.mu.df <- data.frame(qrisk.P = qrisk.P, mu.P = mu.P )
mu.P <- qrisk.mu.df$mu.P
mu.free <-  0.0003 ## input value of risk-free interest rate
sharpe <- ( mu.P-mu.free)/qrisk.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (qrisk.P == min(qrisk.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## find the efficient frontier (blue)
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
weights.extr <- weights[ind,] # for use in calculating tengency risk measures
qrisk.mu.df$col.P <- col.P
renderPlotly({
  p <- ggplot(qrisk.mu.df, aes(x = qrisk.P, y = mu.P, group = 1)) + geom_line(aes(colour= col.P, group = col.P)) + scale_colour_identity()  
  p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
  options(digits=3)
  p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/qrisk.P[ind], colour = "red")
  p <- p + geom_point(aes(x = qrisk.P[ind], y = mu.P[ind])) 
  p <- p + geom_point(aes(x = qrisk.P[ind2], y = mu.P[ind2])) 
  ggplotly(p)
})
```

### Extreme portfolio risk measures

```{r }
# price.last <- as.numeric(tail(data[, -1], n=1))
# Specify the positions
position.rf <- weights.extr #c(1/3, 1/3, 1/3)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(data.r/100) * weights.rf)
alpha.tolerance <- 0.95
u <- quantile(loss.rf, alpha.tolerance, names = FALSE)
fit.extr <- fit.GPD(loss.rf, threshold = u)
renderPlot({
  showRM(fit.extr, alpha = alpha.tolerance, RM = "ES", method = "BFGS")
})
```

### Markowitz efficient portfolio frontier

```{r }
R <- returns[,1:3]/100
names.R <- colnames(R)
mean.R <-  apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R)) ## remember these are in daily percentages
library(quadprog)
Amat <-  cbind(rep(1,3),mean.R)  ## set the equality constraints matrix
mu.P <- seq(-.0005, 0.0015, length = 100)  ## set of 300 possible target portfolio returns
sigma.P <-  mu.P ## set up storage for std dev's of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights
colnames(weights) <- names.R
for (i in 1:length(mu.P))
{
  bvec = c(1,mu.P[i])  ## constraint vector
  result =
    solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] = sqrt(result$value)
  weights[i,] = result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
mu.free <-  .0003 ## input value of risk-free interest rate
sharpe <- ( mu.P-mu.free)/sigma.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P
renderPlotly({
p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind])) 
p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3])
ggplotly(p)
})
```

### Portfolio Weights

The weights for the Markowitz tangency portfolio ("*") are

```{r echo = FALSE}
library(scales)
weights[ind2,][1,]
name <- colnames(weights)
posn <- ifelse((weights[ind,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights[ind,]))
```

The weights for the QR tangency portfolio are

```{r echo = FALSE}
library(scales)
weights.extr[1,]
```

For the working capital accounts:

1. \$250 million denominated in euros 
2. Buy \$166 million in nickel
3. Buy \$40.75 million in copper
4. Buy \$43 million in aluminium


Insights
=======================================================================

row {.tabset}
-------------------------------------------------------------------------
### Key Insights

- The working capital account of \$250 million euro should be allocated as follows: buy \$166 million in nickel, \$40.75 million in copper, \$43 million in aluminium.

- There is a strong correlation (0.89) between nickel and copper due to shared applications.

- Creation of cupronickel alloy which is used for desalinisation due its high resistant to corrosion, minting, armaments, marine engineering, electrical applications, and many others, e.g. the repair of fan blades found in geothermal power plants. 
    
- Another example would be purchasing Copper from Codelco in Santiago, Chile and tramping it to General Electric, Schnectady, NY as part of GE's purchasing and procurement wing of their supply chain.  

- Aluminium is a metal which is used in a plethora of industries and markets.  It's relatively stable price is testament to this.  Moving aluminium from Brasil to West Coast USA for aircraft supply chains is a safe, long term freight line that ship owner's can use to mitigate risk.

Row
------------------------------------------------------------------------
### References

**Data and markets**

- Quandl for data
- Github and Stack Overflow for trouble shooting
- LME.com (London Metals Exchange)

**R Packages used**

ggplot, scales, quadprog, quantreg, shiny, flexdashboard, qrmdata, xts, matrixStats, zoo, QRM, plotly, and psych


